# çµ±ä¸€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°APIç§»è¡Œã‚¬ã‚¤ãƒ‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ—¢å­˜ã®Chromeå°‚ç”¨ã¾ãŸã¯HTTPå°‚ç”¨ã‚³ãƒ¼ãƒ‰ã‚’çµ±ä¸€Action-baseã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«ç§»è¡Œã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ‰ æ–°ã—ã„Action-basedè¨­è¨ˆ

v2.0ã§ã¯ã€chromedp.Run()ãƒ©ã‚¤ã‚¯ãªAction-based APIã«åˆ·æ–°ã•ã‚Œã¾ã—ãŸã€‚chromedpã®å„ªã‚ŒãŸå¯å¤‰å¼•æ•°ã‚¹ã‚¿ã‚¤ãƒ«ã‚’çµ±ä¸€APIã§ã‚‚å®Ÿç¾ã—ã€ã‚ˆã‚Šç°¡æ½”ã§ä¿å®ˆã—ã‚„ã™ã„ã‚³ãƒ¼ãƒ‰ãŒæ›¸ã‘ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

## ç›®æ¬¡

1. [ç§»è¡Œã®åˆ©ç‚¹](#ç§»è¡Œã®åˆ©ç‚¹)
2. [åŸºæœ¬çš„ãªç§»è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³](#åŸºæœ¬çš„ãªç§»è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³)
3. [ãƒ¡ã‚½ãƒƒãƒ‰å¯¾å¿œè¡¨](#ãƒ¡ã‚½ãƒƒãƒ‰å¯¾å¿œè¡¨)
4. [å®Ÿè·µçš„ãªç§»è¡Œä¾‹](#å®Ÿè·µçš„ãªç§»è¡Œä¾‹)
5. [æ³¨æ„ç‚¹ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](#æ³¨æ„ç‚¹ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹)

## ç§»è¡Œã®åˆ©ç‚¹

### âœ… **ã‚³ãƒ¼ãƒ‰ã®çµ±ä¸€åŒ–**

- Chromeç‰ˆã¨HTTPç‰ˆã§åŒã˜ã‚³ãƒ¼ãƒ‰ãŒä½¿ç”¨å¯èƒ½
- å®Ÿè¡Œæ™‚ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ–¹æ³•ã‚’åˆ‡ã‚Šæ›¿ãˆå¯èƒ½
- ãƒ†ã‚¹ãƒˆç’°å¢ƒï¼ˆHTTPï¼‰ã¨æœ¬ç•ªç’°å¢ƒï¼ˆChromeï¼‰ã®åˆ‡ã‚Šæ›¿ãˆãŒå®¹æ˜“

### âœ… **æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®ä¿è­·**

- æ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã™ã¹ã¦ä¿æŒ
- æ®µéšçš„ãªç§»è¡ŒãŒå¯èƒ½
- å¾Œæ–¹äº’æ›æ€§ã‚’å®Œå…¨ã«ç¶­æŒ

## åŸºæœ¬çš„ãªç§»è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³

### ãƒ‘ã‚¿ãƒ¼ãƒ³1: chromedp.Run()ã‚¹ã‚¿ã‚¤ãƒ«ã®ç§»è¡Œ

**ç§»è¡Œå‰ï¼ˆChromeå°‚ç”¨ï¼‰:**

```go
func scrapeWithChrome(session *scraper.Session) error {
    chromeSession, cancel, err := session.NewChrome()
    if err != nil {
        return err
    }
    defer cancel()

    err = chromedp.Run(chromeSession.Ctx,
        chromedp.Navigate("https://example.com"),
        chromedp.WaitVisible("h1", chromedp.ByQuery),
        chromeSession.SaveHtml(nil),
    )
    if err != nil {
        return err
    }

    var data struct {
        Title string `find:"h1"`
    }
    return chromeSession.Unmarshal(&data, "body", scraper.UnmarshalOption{})
}
```

**ç§»è¡Œå¾Œï¼ˆçµ±ä¸€Action-based APIï¼‰:**

```go
func scrapeUnified(scraperInstance scraper.UnifiedScraper) error {
    // chromedp.Run()ã¨åŒã˜ã‚¹ã‚¿ã‚¤ãƒ«ã§çµ±ä¸€APIä½¿ç”¨
    err := scraperInstance.Run(
        scraper.Navigate("https://example.com"),
        scraper.WaitVisible("h1"),
        scraper.SavePage(),
    )
    if err != nil {
        return err
    }

    // ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚‚çµ±ä¸€
    var data struct {
        Title string `find:"h1"`
    }
    return scraperInstance.Run(
        scraper.ExtractData(&data, "body", scraper.UnmarshalOption{}),
    )
}
```

### ãƒ‘ã‚¿ãƒ¼ãƒ³2: å®Ÿè¡Œæ™‚åˆ‡ã‚Šæ›¿ãˆå¯¾å¿œ

**ç§»è¡Œå‰ï¼ˆå›ºå®šæ–¹å¼ï¼‰:**

```go
func main() {
    var logger scraper.ConsoleLogger
    session := scraper.NewSession("test", logger)
    
    // å¸¸ã«Chromeä½¿ç”¨
    chromeSession, cancel, err := session.NewChrome()
    if err != nil {
        log.Fatal(err)
    }
    defer cancel()
    
    scrapeWithChrome(session)
}
```

**ç§»è¡Œå¾Œï¼ˆè¨­å®šå¯èƒ½ï¼‰:**

```go
func main() {
    useChrome := flag.Bool("chrome", false, "Use Chrome scraping")
    flag.Parse()
    
    var logger scraper.ConsoleLogger
    session := scraper.NewSession("test", logger)
    
    var scraperInstance scraper.UnifiedScraper
    var cancel context.CancelFunc
    
    if *useChrome {
        chromeSession, c, err := session.NewChrome()
        if err != nil {
            log.Fatal(err)
        }
        scraperInstance = chromeSession
        cancel = c
        defer cancel()
    } else {
        scraperInstance = session
    }
    
    scrapeUnified(scraperInstance)
}
```

## Actionå¯¾å¿œè¡¨

### Chromeå°‚ç”¨ã‚³ãƒ¼ãƒ‰ â†’ çµ±ä¸€Action API

| ç§»è¡Œå‰ | ç§»è¡Œå¾Œ | èª¬æ˜ |
|--------|--------|------|
| `chromedp.Navigate(url)` | `scraper.Navigate(url)` | ãƒšãƒ¼ã‚¸é·ç§» |
| `chromedp.WaitVisible(sel)` | `scraper.WaitVisible(sel)` | è¦ç´ ã®è¡¨ç¤ºå¾…ã¡ |
| `chromedp.SendKeys(sel, val)` | `scraper.SendKeys(sel, val)` | ãƒ•ã‚©ãƒ¼ãƒ å…¥åŠ› |
| `chromedp.Click(sel)` | `scraper.Click(sel)` | ã‚¯ãƒªãƒƒã‚¯æ“ä½œ |
| `chromedp.Sleep(duration)` | `scraper.Sleep(duration)` | å¾…æ©Ÿï¼ˆreplayæ™‚è‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—ï¼‰ |
| `chromeSession.SaveHtml(nil)` | `scraper.SavePage()` | HTMLä¿å­˜ |
| `chromeSession.Unmarshal(&v, sel, opt)` | `scraper.ExtractData(&v, sel, opt)` | ãƒ‡ãƒ¼ã‚¿æŠ½å‡º |

### å®Ÿè¡Œæ–¹æ³•ã®æ¯”è¼ƒ

**Chromeå°‚ç”¨ (ç§»è¡Œå‰):**
```go
err = chromedp.Run(ctx,
    chromedp.Navigate(url),
    chromedp.WaitVisible(sel),
    chromedp.Click(sel),
)
```

**çµ±ä¸€Action API (ç§»è¡Œå¾Œ):**
```go
err = scraper.Run(
    scraper.Navigate(url),
    scraper.WaitVisible(sel),
    scraper.Click(sel),
)
```

### HTTPå°‚ç”¨ã‚³ãƒ¼ãƒ‰ â†’ çµ±ä¸€Action API

HTTPã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ã‚‚åŒã˜Actionã‚’ä½¿ç”¨ã§ãã¾ã™ï¼š

| ç§»è¡Œå‰ | ç§»è¡Œå¾Œ | èª¬æ˜ |
|--------|--------|------|
| `session.GetPage(url)` + çŠ¶æ…‹ç®¡ç† | `scraper.Navigate(url)` | ãƒšãƒ¼ã‚¸å–å¾—ã¨çŠ¶æ…‹æ›´æ–° |
| `session.FormAction(page, sel, params)` | `scraper.SendKeys()` + `scraper.Click()` | ãƒ•ã‚©ãƒ¼ãƒ æ“ä½œã‚’åˆ†å‰² |
| `session.FollowAnchorText(page, text)` | `scraper.Click()` with text selector | ãƒªãƒ³ã‚¯ã‚¯ãƒªãƒƒã‚¯ |
| `scraper.Unmarshal(&v, selection, opt)` | `scraper.ExtractData(&v, sel, opt)` | ãƒ‡ãƒ¼ã‚¿æŠ½å‡º |

## å®Ÿè·µçš„ãªç§»è¡Œä¾‹

### ä¾‹1: SBIè¨¼åˆ¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ç§»è¡Œ

**ç§»è¡Œå‰ã®Chromeå°‚ç”¨ã‚³ãƒ¼ãƒ‰:**

```go
func getSbiSecurityChrome(param ParamRegistry, service StatementReceiver, session *scraper.Session) error {
    chromeSession, cancel, err := session.NewChrome()
    if err != nil {
        return err
    }
    defer cancel()
    
    err = chromedp.Run(chromeSession.Ctx,
        chromedp.Navigate(`https://www.sbisec.co.jp/`),
        chromedp.WaitVisible(`form[name=form_login]`, chromedp.ByQuery),
        chromedp.SendKeys(`input[name=user_id]`, param.Param(ParamUser), chromedp.ByQuery),
        chromedp.SendKeys(`input[name=user_password]`, param.Param(ParamPassword), chromedp.ByQuery),
        chromedp.Click(`[name=ACT_login]`, chromedp.ByQuery),
    )
    if err != nil {
        return err
    }
    
    // CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†...
    return nil
}
```

**ç§»è¡Œå¾Œã®çµ±ä¸€Action APIç‰ˆ:**

```go
func getSbiSecurityUnified(param ParamRegistry, service StatementReceiver, scraperInstance scraper.UnifiedScraper) error {
    scraperInstance.SetDebugStep("SBIè¨¼åˆ¸ãƒ­ã‚°ã‚¤ãƒ³")
    defer scraperInstance.ClearDebugStep()

    // ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†ã‚’Action-baseã‚¹ã‚¿ã‚¤ãƒ«ã§å®Ÿè¡Œ
    err := scraperInstance.Run(
        scraper.Navigate(`https://www.sbisec.co.jp/`),
        scraper.WaitVisible(`form[name=form_login]`),
        scraper.SendKeys(`input[name=user_id]`, param.Param(ParamUser)),
        scraper.SendKeys(`input[name=user_password]`, param.Param(ParamPassword)),
        scraper.Click(`[name=ACT_login]`),
        scraper.SavePage(),
    )
    if err != nil {
        return err
    }

    // ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå‡¦ç†
    type PositionData struct {
        Items []struct {
            Name  string  `find:".stock-name"`
            Price float64 `find:".price" re:"([0-9,]+)"`
        } `find:".position-row"`
    }

    var positions PositionData
    err = scraperInstance.Run(
        scraper.Click("a:contains('ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª')"), // ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒšãƒ¼ã‚¸ã¸
        scraper.WaitVisible(".portfolio-table"),
        scraper.ExtractData(&positions, ".portfolio-table", scraper.UnmarshalOption{}),
    )
    if err != nil {
        return err
    }

    scraperInstance.Printf("å–å¾—ã—ãŸãƒã‚¸ã‚·ãƒ§ãƒ³æ•°: %d", len(positions.Items))
    return nil
}
```

### ä¾‹2: å‘¼ã³å‡ºã—å´ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³

```go
type ScraperConfig struct {
    UseChrome bool
    Headless  bool
    Timeout   time.Duration
}

func createScraper(config ScraperConfig, session *scraper.Session) (scraper.UnifiedScraper, context.CancelFunc, error) {
    if config.UseChrome {
        options := scraper.NewChromeOptions{
            Headless: config.Headless,
            Timeout:  config.Timeout,
        }
        chromeSession, cancel, err := session.NewChromeOpt(options)
        if err != nil {
            return nil, nil, err
        }
        return chromeSession, cancel, nil
    } else {
        // HTTPç‰ˆã¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ä¸è¦
        return session, func() {}, nil
    }
}

func main() {
    config := ScraperConfig{
        UseChrome: os.Getenv("USE_CHROME") == "true",
        Headless:  true,
        Timeout:   30 * time.Second,
    }
    
    var logger scraper.ConsoleLogger
    session := scraper.NewSession("unified-example", logger)
    
    scraperInstance, cancel, err := createScraper(config, session)
    if err != nil {
        log.Fatal(err)
    }
    defer cancel()
    
    // çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§å‡¦ç†
    err = getSbiSecurityUnified(param, service, scraperInstance)
    if err != nil {
        log.Fatal(err)
    }
}
```

## æ–°æ©Ÿèƒ½ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ğŸ¯ **Action-based APIã®åˆ©ç‚¹**

#### 1. **ã‚«ã‚¹ã‚¿ãƒ Actionä½œæˆ**

```go
// ã‚ˆãä½¿ã†æ“ä½œã‚’Actionã¨ã—ã¦éƒ¨å“åŒ–
func JCBLogin(userId, password string) scraper.UnifiedAction {
    return scraper.ActionFunc(func(s scraper.UnifiedScraper) error {
        return s.Run(
            scraper.Navigate("https://my.jcb.co.jp/Login"),
            scraper.WaitVisible("form[name='loginForm']"),
            scraper.SendKeys("#userId", userId),
            scraper.SendKeys("#password", password),
            scraper.Sleep(2*time.Second),
            scraper.Click("#loginButtonAD"),
        )
    })
}

// ä½¿ç”¨ä¾‹
err := scraper.Run(
    JCBLogin("myuser", "mypass"),
    scraper.SavePage(),
    // ç¶šãã®å‡¦ç†...
)
```

#### 2. **æ¡ä»¶åˆ†å²å‡¦ç†**

```go
// Actionå†…ã§æ¡ä»¶åˆ†å²ã‚‚å¯èƒ½
func ConditionalLogin(scraper scraper.UnifiedScraper) error {
    err := scraper.Run(
        scraper.Navigate("https://example.com/login"),
        scraper.SavePage(),
    )
    if err != nil {
        return err
    }

    // ç¾åœ¨ã®URLã§å‡¦ç†ã‚’åˆ†å²
    currentURL, _ := scraper.GetCurrentURL()
    if strings.Contains(currentURL, "yahoo.co.jp") {
        return scraper.Run(
            scraper.WaitVisible(`input[name="handle"]`),
            scraper.SendKeys(`input[name="handle"]`, userId),
            scraper.Click(`button[class*="riff-bg-key"]`),
        )
    } else {
        return scraper.Run(
            scraper.WaitVisible("#loginForm"),
            scraper.SendKeys("#username", userId),
            scraper.Click("#submit"),
        )
    }
}
```

#### 3. **Replay Modeå®Œå…¨å¯¾å¿œ**

```go
// Sleep ã¯ replay mode ã§è‡ªå‹•çš„ã«ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹
err := scraper.Run(
    scraper.Navigate("https://example.com"),
    scraper.Sleep(3*time.Second), // è¨˜éŒ²æ™‚ã®ã¿å®Ÿè¡Œã€ãƒªãƒ—ãƒ¬ã‚¤æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
    scraper.Click("#button"),
)

// IsReplayMode()ã§çŠ¶æ…‹ç¢ºèªã‚‚å¯èƒ½
if !scraper.IsReplayMode() {
    scraper.Printf("å®Ÿéš›ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€šä¿¡ä¸­...")
}
```

### âœ… **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**

#### 1. **æ®µéšçš„ç§»è¡Œ**

```go
// Phase 1: çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å—ã‘å–ã‚Œã‚‹ã‚ˆã†ã«å¤‰æ›´
func processData(scraperInstance scraper.UnifiedScraper) error {
    // çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ãŸå‡¦ç†
}

// Phase 2: æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å¾ã€…ã«ç§»è¡Œ
func oldFunction(session *scraper.Session) error {
    return processData(session) // æ—¢å­˜ã®Sessionã‚‚çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¨ã—ã¦æ¸¡ã›ã‚‹
}
```

#### 2. **è¨­å®šé§†å‹•ã®åˆ‡ã‚Šæ›¿ãˆ**

```go
type Config struct {
    ScrapingMode string `json:"scraping_mode"` // "http" or "chrome"
    Chrome       struct {
        Headless bool          `json:"headless"`
        Timeout  time.Duration `json:"timeout"`
    } `json:"chrome"`
}

func createScraperFromConfig(config Config, session *scraper.Session) scraper.UnifiedScraper {
    switch config.ScrapingMode {
    case "chrome":
        chromeSession, _, _ := session.NewChromeOpt(scraper.NewChromeOptions{
            Headless: config.Chrome.Headless,
            Timeout:  config.Chrome.Timeout,
        })
        return chromeSession
    default:
        return session
    }
}
```

#### 3. **ãƒ†ã‚¹ãƒˆæˆ¦ç•¥**

```go
func TestUnifiedScraping(t *testing.T) {
    testCases := []struct {
        name    string
        scraper scraper.UnifiedScraper
    }{
        {
            name:    "HTTP Scraper",
            scraper: scraper.NewSession("test-http", logger),
        },
        {
            name:    "Chrome Scraper", 
            scraper: setupChromeSession(t),
        },
    }
    
    for _, tc := range testCases {
        t.Run(tc.name, func(t *testing.T) {
            err := processData(tc.scraper)
            assert.NoError(t, err)
        })
    }
}
```

#### 4. **æ—¢å­˜æ©Ÿèƒ½ã®æ´»ç”¨**

```go
func advancedDownload(scraperInstance scraper.UnifiedScraper) error {
    // çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã‚·ãƒ³ãƒ—ãƒ«ãªã‚±ãƒ¼ã‚¹ã‚’å‡¦ç†
    filename, err := scraperInstance.DownloadResource(options)
    if err != nil {
        // å¤±æ•—ã—ãŸå ´åˆã¯å‹å›ºæœ‰ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨
        if chromeSession, ok := scraperInstance.(*scraper.ChromeSession); ok {
            return chromeSession.DownloadFile(&filename, chromeOptions, 
                chromedp.Click(".download-button"),
            )
        }
        return err
    }
    return nil
}
```

## ã¾ã¨ã‚

Action-basedçµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¸ã®ç§»è¡Œã«ã‚ˆã‚Šï¼š

1. **ğŸ¯ chromedp.Run()ã‚¹ã‚¿ã‚¤ãƒ«ã§ç›´æ„Ÿçš„ãªæ“ä½œ**
2. **ğŸ”„ Chromeç‰ˆã¨HTTPç‰ˆã®ã‚³ãƒ¼ãƒ‰ãŒå®Œå…¨çµ±ä¸€**
3. **âš¡ Replay Mode ã§çˆ†é€Ÿé–‹ç™ºãƒ»ãƒ‡ãƒãƒƒã‚°**
4. **ğŸ§© ã‚«ã‚¹ã‚¿ãƒ Actionã§é«˜ã„å†åˆ©ç”¨æ€§**
5. **ğŸ› ï¸ æ¡ä»¶åˆ†å²ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒç°¡æ½”**

## ç§»è¡Œã®ãƒã‚¤ãƒ³ãƒˆ

### âœ… **ã™ãã«ç§»è¡Œã™ã¹ãç†ç”±**

- **é–‹ç™ºåŠ¹ç‡ã®åŠ‡çš„å‘ä¸Š**: Replay modeã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹ç™ºãŒçˆ†é€ŸåŒ–
- **ã‚³ãƒ¼ãƒ‰ä¿å®ˆæ€§**: chromedp.Run()ã‚¹ã‚¿ã‚¤ãƒ«ã§å¯èª­æ€§ã‚¢ãƒƒãƒ—
- **ãƒ†ã‚¹ãƒˆå®¹æ˜“æ€§**: HTTP/Chromeä¸¡æ–¹ã§åŒã˜ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
- **æ¡ä»¶åˆ†å²ã®ç°¡æ½”æ€§**: ActionFuncãŒä¸è¦ã«ãªã‚ŠGoã®æ¨™æº–åˆ¶å¾¡æ§‹æ–‡ã§è¨˜è¿°

### ğŸš€ **æ¨å¥¨ç§»è¡Œæ‰‹é †**

1. **æ–°æ©Ÿèƒ½ã¯Action-based APIã§å®Ÿè£…**
2. **æ—¢å­˜ã®å•é¡Œç®‡æ‰€ã‹ã‚‰æ®µéšçš„ã«ç§»è¡Œ**
3. **Replay modeã‚’æ´»ç”¨ã—ã¦é–‹ç™ºã‚’é«˜é€ŸåŒ–**
4. **ã‚«ã‚¹ã‚¿ãƒ Actionã§å…±é€šå‡¦ç†ã‚’éƒ¨å“åŒ–**

ã“ã®æ–°è¨­è¨ˆã«ã‚ˆã‚Šã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ã®é–‹ç™ºãƒ»ä¿å®ˆãƒ»ãƒ†ã‚¹ãƒˆãŒæ ¼æ®µã«åŠ¹ç‡åŒ–ã•ã‚Œã¾ã™ï¼
